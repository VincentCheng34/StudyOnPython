{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Volumes/backup_128G/z_repository/Yang_data'\n",
    "read_experiment_path = '{0}/experiment-20190511T105119Z-001'.format(filepath)\n",
    "read_comparison_path = '{0}/對照組_前測_逐字稿-20190511T105456Z-001'.format(filepath)\n",
    "\n",
    "file_dic = '7000Words_20190512_v2.xlsx'\n",
    "to_exp_doc = 'experiment_20190512_v1.xlsx'\n",
    "to_com_doc = 'comparison_20190512_v1.xlsx'\n",
    "to_level_doc = 'level_20190514_v1.xlsx'\n",
    "\n",
    "write_exp_doc = '{0}/{1}'.format(filepath, to_exp_doc)\n",
    "write_com_doc = '{0}/{1}'.format(filepath, to_com_doc)\n",
    "write_level_doc = '{0}/{1}'.format(filepath, to_level_doc)\n",
    "read_dic = '{0}/{1}'.format(filepath, file_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6813"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicDf = pd.DataFrame()\n",
    "\n",
    "with pd.ExcelFile(read_dic) as reader:\n",
    "    # read sheet by sheet\n",
    "    for sheet in reader.sheet_names:\n",
    "#         print(sheet)\n",
    "        sheetDf = pd.read_excel(reader, sheet, header=None)\n",
    "        sheetDf = sheetDf.fillna(0)\n",
    "\n",
    "        dicDf = dicDf.append(sheetDf, ignore_index=True)\n",
    "\n",
    "# change to lowercase\n",
    "dicDf[0] = dicDf[0].str.lower()\n",
    "len(dicDf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>art.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an</td>\n",
       "      <td>art.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>v.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>v.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbreviation</td>\n",
       "      <td>n.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1  2\n",
       "0             a  art.  1\n",
       "1            an  art.  1\n",
       "2       abandon    v.  4\n",
       "3    abbreviate    v.  6\n",
       "4  abbreviation    n.  6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將英文字 lemmatize（詞形還原）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞性還原 \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wtlem = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def findWord(word, dicDataF):\n",
    "    found = dicDataF.loc[dicDataF[0] == word]\n",
    "    if found.empty == False:\n",
    "        return found.iloc[0][2]\n",
    "    return None\n",
    "    \n",
    "def returnWord(word, level):\n",
    "    return {'lemma':word, 'level':level}\n",
    "\n",
    "def lemmatizer(word, dicDataF):\n",
    "    lowerWord = word.lower()\n",
    "    \n",
    "    found = findWord(lowerWord, dicDataF)\n",
    "    if found != None:\n",
    "        return returnWord(word, found)\n",
    "    \n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.ADJ)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.VERB)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "        \n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.NOUN)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.ADV)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    return returnWord(word, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀檔；去掉非英文字，回傳 word 字串，包含引用次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-docx \n",
    "import docx\n",
    "\n",
    "def isAlpha(word):\n",
    "    try:\n",
    "        return word.encode('ascii').isalpha()\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    \n",
    "def wordCount(wordList):\n",
    "    wCount = {}\n",
    "    for word in wordList:\n",
    "        if word not in wCount:\n",
    "            wCount[word] = 1\n",
    "        else:\n",
    "            wCount[word] += 1\n",
    "    return wCount\n",
    "    \n",
    "def split2Words(txt):\n",
    "    wordList = []\n",
    "    cleanTxt = txt.replace(',', '').replace('.', '')\n",
    "    splitWords = cleanTxt.split(' ')\n",
    "    for word in splitWords:\n",
    "        if '’' in word or '-' in word:\n",
    "            wordList.append(word)\n",
    "            continue\n",
    "\n",
    "        if isAlpha(word) == True:\n",
    "            wordList.append(word)\n",
    "            \n",
    "    return wordCount(wordList)\n",
    "    \n",
    "def replaceMultiple(mainString, toBeReplaces, newString):\n",
    "#     outString = copy.copy(mainString)\n",
    "    # Iterate over the strings to be replaced\n",
    "    for elem in toBeReplaces :\n",
    "        # Check if string is in the main string\n",
    "        if elem in mainString :\n",
    "            # Replace the string\n",
    "            mainString = mainString.replace(elem, newString)\n",
    "    \n",
    "    return mainString\n",
    "\n",
    "def readtxt(filename):\n",
    "    fullText = []\n",
    "    doc = docx.Document(filename)\n",
    "    tables = doc.tables\n",
    "    for table in tables:\n",
    "        for row in table.rows:\n",
    "            for cell in row.cells:\n",
    "                for paragraph in cell.paragraphs:\n",
    "                    processedPara = replaceMultiple(paragraph.text, ['…', '’', '‘', '-'] , ' ')\n",
    "#                     print(processedPara)\n",
    "                    fullText.append(processedPara)\n",
    "#                     print(paragraph.text)\n",
    "\n",
    "    fullText = ' '.join(fullText)\n",
    "    return split2Words(fullText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀檔列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDoc(wordList):\n",
    "    resWordList = {}\n",
    "    for word in wordList:\n",
    "        lowerWord = word.lower()\n",
    "        worddic = lemmatizer(lowerWord, dicDf)\n",
    "        if worddic['lemma'] not in resWordList:\n",
    "            resWordList[worddic['lemma']] = {'words':word, 'count':wordList[word], 'level':worddic['level']}\n",
    "        else:\n",
    "            resWordList[worddic['lemma']]['words'] += ';{0}'.format(word)\n",
    "            resWordList[worddic['lemma']]['count'] += wordList[word]\n",
    "            if resWordList[worddic['lemma']]['level'] != worddic['level']:\n",
    "                print(\"ERROR!!\", word, worddic)\n",
    "\n",
    "    #     print(word, lemmatizer(word, dicDf))\n",
    "    return resWordList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP05-post.docx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EXP05-post-num</th>\n",
       "      <td>140</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1   2  3  4  5  6\n",
       "EXP05-post-num  140  26  5  1  0  1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"{0}/EXP05-post.docx\".format(read_experiment_path)\n",
    "print(filename)\n",
    "wordList = readtxt(filename)\n",
    "processedList = processDoc(wordList)\n",
    "\n",
    "# output\n",
    "outDf = pd.DataFrame(processedList).T\n",
    "outDf.sort_values(by=['level', 'words'], inplace=True)\n",
    "\n",
    "# list level words\n",
    "levelDf = pd.DataFrame(columns=list('123456'))\n",
    "numWords = {}\n",
    "# levelWords = {}\n",
    "for level in range(1, 7):\n",
    "    words = outDf.loc[outDf['level'] == level].index\n",
    "    numWords[level] = len(words)\n",
    "#     levelWords[level] = ';'.join(words)\n",
    "    \n",
    "levelDf = pd.DataFrame(numWords, index=['EXP05-post-num'])\n",
    "# levelDf = pd.DataFrame(levelWords, index=['EXP05-post'])\n",
    "levelDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP01-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP02-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP03-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP04-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP05-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP06-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP07-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP08-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP09-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP10-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP11-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP12-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP13-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP14-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP15-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/experiment-20190511T105119Z-001/EXP16-post.docx\n"
     ]
    }
   ],
   "source": [
    "levelColumns = []\n",
    "levelNums = []\n",
    "levelWords = []\n",
    "\n",
    "with pd.ExcelWriter(write_exp_doc) as writer:\n",
    "    for r, d, f in os.walk(read_experiment_path):\n",
    "        for file in f:\n",
    "            if '.docx' in file and file[0] != '.':\n",
    "                sheetName = file.replace('.docx', '')\n",
    "\n",
    "                filename = os.path.join(r, file)\n",
    "                print(filename)\n",
    "                wordList = readtxt(filename)\n",
    "                processedList = processDoc(wordList)\n",
    "\n",
    "                # output\n",
    "                outDf = pd.DataFrame(processedList).T\n",
    "                outDf.sort_values(by=['level', 'words'], inplace=True)\n",
    "    #             outDf.head()\n",
    "    \n",
    "                # list level words\n",
    "                levelColumns.append(sheetName)\n",
    "        \n",
    "                numWords = {}\n",
    "                wordWords = {}\n",
    "                for level in range(1, 7):\n",
    "                    words = outDf.loc[outDf['level'] == level].index\n",
    "                    numWords[level] = len(words)\n",
    "                    wordWords[level] = ';'.join(words)\n",
    "                \n",
    "                levelNums.append(numWords)\n",
    "                levelWords.append(wordWords)\n",
    "\n",
    "                # write file\n",
    "                outDf.to_excel(writer, sheetName)\n",
    "                writer.save()\n",
    "\n",
    "expLevelNumDf = pd.DataFrame(levelNums, index=levelColumns)\n",
    "expLevelWordsDf = pd.DataFrame(levelWords, index=levelColumns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/01余貞儀_陽翟(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/02李晟焱_陳景蘭(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/03汪鈺翔_慈湖(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/04施丞壕_小金門(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/05洪小童_慈湖(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/06張韻廷_山后(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/07曹祐瑞_山后(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/09陳怡君_陳景蘭(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/11陳羚婷_馬山(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/12陳琬鈞_馬山(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/13陳裕萱_小金門(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/14樊佩潤_陽翟(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/15歐陽湘芸_陳景蘭(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/16賴沛渝_陽翟(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/17王朝胤_馬山(前).docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/對照組_前測_逐字稿-20190511T105456Z-001/20陳芸萱_小金門(前).docx\n"
     ]
    }
   ],
   "source": [
    "levelColumns = []\n",
    "levelNums = []\n",
    "levelWords = []\n",
    "\n",
    "with pd.ExcelWriter(write_com_doc) as writer:\n",
    "    for r, d, f in os.walk(read_comparison_path):\n",
    "        for file in f:\n",
    "            if '.docx' in file and file[0] != '.':\n",
    "                sheetName = file.replace('.docx', '')\n",
    "\n",
    "                filename = os.path.join(r, file)\n",
    "                print(filename)\n",
    "                wordList = readtxt(filename)\n",
    "                processedList = processDoc(wordList)\n",
    "\n",
    "                # output\n",
    "                outDf = pd.DataFrame(processedList).T\n",
    "                outDf.sort_values(by=['level', 'words'], inplace=True)\n",
    "    #             outDf.head()\n",
    "    \n",
    "                # list level words\n",
    "                levelColumns.append(sheetName)\n",
    "            \n",
    "                numWords = {}\n",
    "                wordWords = {}\n",
    "                for level in range(1, 7):\n",
    "                    words = outDf.loc[outDf['level'] == level].index\n",
    "                    numWords[level] = len(words)\n",
    "                    wordWords[level] = ';'.join(words)\n",
    "                \n",
    "                levelNums.append(numWords)\n",
    "                levelWords.append(wordWords)\n",
    "                \n",
    "                # write file\n",
    "                outDf.to_excel(writer, sheetName)\n",
    "                writer.save()\n",
    "                \n",
    "compLevelNumDf = pd.DataFrame(levelNums, index=levelColumns)\n",
    "comLevelWordsDf = pd.DataFrame(levelWords, index=levelColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write level words/nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expLevelNumDf\n",
    "# expLevelWordsDf\n",
    "# compLevelNumDf\n",
    "# comLevelWordsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(write_level_doc) as writer:\n",
    "    # write file\n",
    "    expLevelNumDf.to_excel(writer, \"EXP Num\")\n",
    "    expLevelWordsDf.to_excel(writer, \"EXP Words\")\n",
    "    compLevelNumDf.to_excel(writer, \"COMP Num\")\n",
    "    comLevelWordsDf.to_excel(writer, \"COMP Words\")\n",
    "    \n",
    "    writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
