{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = '/Volumes/backup_128G/z_repository/Yang_data/學測指考英文'\n",
    "dataPath = 'data_docx'\n",
    "outputPath = 'output'\n",
    "\n",
    "from_dataPath = '{0}/{1}'.format(basePath, dataPath)\n",
    "to_dataPath = '{0}/{1}'.format(basePath, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dictionary for level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6813"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dic = 'permantData/7000Words_20190512_v2.xlsx'\n",
    "read_dic = '{0}/{1}'.format(basePath, file_dic)\n",
    "\n",
    "dicDf = pd.DataFrame()\n",
    "\n",
    "with pd.ExcelFile(read_dic) as reader:\n",
    "    # read sheet by sheet\n",
    "    for sheet in reader.sheet_names:\n",
    "#         print(sheet)\n",
    "        sheetDf = pd.read_excel(reader, sheet, header=None)\n",
    "        sheetDf = sheetDf.fillna(0)\n",
    "\n",
    "        dicDf = dicDf.append(sheetDf, ignore_index=True)\n",
    "\n",
    "# change to lowercase\n",
    "dicDf[0] = dicDf[0].str.lower()\n",
    "len(dicDf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>art.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an</td>\n",
       "      <td>art.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>v.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>v.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbreviation</td>\n",
       "      <td>n.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1  2\n",
       "0             a  art.  1\n",
       "1            an  art.  1\n",
       "2       abandon    v.  4\n",
       "3    abbreviate    v.  6\n",
       "4  abbreviation    n.  6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only care of levels 5 and 6 as high level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>v.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>v.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbreviation</td>\n",
       "      <td>n.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abdomen</td>\n",
       "      <td>n.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abide</td>\n",
       "      <td>v.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   1  2\n",
       "2       abandon  v.  4\n",
       "3    abbreviate  v.  6\n",
       "4  abbreviation  n.  6\n",
       "5       abdomen  n.  4\n",
       "6         abide  v.  5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlevels = [2, 3, 4, 5, 6]\n",
    "dicHighLevel_TF = dicDf[2].isin(highlevels)\n",
    "dicHighLevel = dicDf[dicHighLevel_TF]\n",
    "dicHighLevel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將英文字 lemmatize（詞形還原）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞性還原 \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wtlem = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def findWord(word, dicDataF):\n",
    "    found = dicDataF.loc[dicDataF[0] == word]\n",
    "    if found.empty == False:\n",
    "        return found.iloc[0][2]\n",
    "    return None\n",
    "    \n",
    "def returnWord(word, level):\n",
    "    return {'lemma':word, 'level':level}\n",
    "\n",
    "def lemmatizer(word, dicDataF):\n",
    "    lowerWord = word.lower()\n",
    "    \n",
    "    found = findWord(lowerWord, dicDataF)\n",
    "    if found != None:\n",
    "        return returnWord(word, found)\n",
    "    \n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.ADJ)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.VERB)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "        \n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.NOUN)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.ADV)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    return returnWord(word, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀檔；去掉非英文字，回傳 word 字串，包含引用次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-docx \n",
    "import docx\n",
    "\n",
    "def isAlpha(word):\n",
    "    try:\n",
    "        return word.encode('ascii').isalpha()\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    \n",
    "def isEnglish(s):\n",
    "    return s.isalpha()\n",
    "    \n",
    "def wordCount(wordList):\n",
    "    wCount = {}\n",
    "    for word in wordList:\n",
    "        if word not in wCount:\n",
    "            wCount[word] = 1\n",
    "        else:\n",
    "            wCount[word] += 1\n",
    "    return wCount\n",
    "    \n",
    "def split2Words(txt):\n",
    "    wordList = []\n",
    "#     cleanTxt = txt.replace(',', '').replace('.', '')\n",
    "    splitWords = txt.split()\n",
    "#     print(splitWords)\n",
    "#     splitWords = [x for x in splitWords if ' ' not in x]\n",
    "    for word in splitWords:\n",
    "#         if '’' in word or '-' in word:\n",
    "#             wordList.append(word)\n",
    "#             continue\n",
    "\n",
    "        if isAlpha(word) == True:\n",
    "            wordList.append(word)\n",
    "        elif word != '':\n",
    "            print(word, ' includes un-alpha characters.')\n",
    "            \n",
    "    return wordCount(wordList)\n",
    "    \n",
    "def replaceMultiple(mainString, toBeReplaces, newString):\n",
    "#     outString = copy.copy(mainString)\n",
    "    # Iterate over the strings to be replaced\n",
    "    for elem in toBeReplaces :\n",
    "        # Check if string is in the main string\n",
    "        if elem in mainString :\n",
    "            # Replace the string\n",
    "            mainString = mainString.replace(elem, newString)\n",
    "    \n",
    "    return mainString\n",
    "\n",
    "def readTxt(filename):\n",
    "    fullText = []\n",
    "    # #VINCENT#, 20190917, To highlight the high level words\n",
    "    plainTxt = []\n",
    "    doc = docx.Document(filename)\n",
    "    wordCnt = 0\n",
    "    for paragraph in doc.paragraphs:\n",
    "#         print(wordCnt, paragraph.text)\n",
    "        if len(paragraph.text.replace(' ', '')) == 0:\n",
    "            continue\n",
    "        wordCnt += len(paragraph.text.split(' '))\n",
    "        \n",
    "        # #VINCENT#, 20190917, To highlight the high level words\n",
    "        plainTxt.append(paragraph.text)\n",
    "        processedPara = replaceMultiple(paragraph.text,\n",
    "                                        ['…', '’', '‘', '\\'', '-', '—', '－', '”', '“', '/',\n",
    "                                         ';', '!', '.', '?', ',', ':', ':', '(', ')', '–'] , ' ')\n",
    "        if len(processedPara) == 0:\n",
    "            continue\n",
    "        fullText.append(processedPara)\n",
    "\n",
    "#     print(wordCnt)\n",
    "    fullText = ' '.join(fullText)\n",
    "    # #VINCENT#, 20190917, To highlight the high level words\n",
    "    return wordCnt, plainTxt, split2Words(fullText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀檔列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDoc(wordList):\n",
    "    resWordList = []\n",
    "    for word in wordList:\n",
    "        lowerWord = word.lower()\n",
    "        worddic = lemmatizer(lowerWord, dicDf)\n",
    "        \n",
    "        # only care of high level words\n",
    "        if worddic['level'] not in highlevels:\n",
    "            continue\n",
    "\n",
    "        if word not in resWordList:\n",
    "            resWordList.append(word)\n",
    "\n",
    "    #     print(word, lemmatizer(word, dicDf))\n",
    "    return resWordList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "from docx.enum.dml import MSO_THEME_COLOR\n",
    "from docx.shared import RGBColor\n",
    "\n",
    "fontColorMap = [WD_COLOR_INDEX.YELLOW, WD_COLOR_INDEX.YELLOW, WD_COLOR_INDEX.RED, \n",
    "            WD_COLOR_INDEX.GREEN, WD_COLOR_INDEX.BLUE, WD_COLOR_INDEX.VIOLET, \n",
    "            WD_COLOR_INDEX.DARK_RED]\n",
    "\n",
    "# colorMap = [MSO_THEME_COLOR.ACCENT_6, MSO_THEME_COLOR.ACCENT_4, MSO_THEME_COLOR.ACCENT_5, \n",
    "#             MSO_THEME_COLOR.ACCENT_6, MSO_THEME_COLOR.ACCENT_3, MSO_THEME_COLOR.ACCENT_2, \n",
    "#             MSO_THEME_COLOR.ACCENT_1]\n",
    "\n",
    "colorMap = [RGBColor(0xff, 0x00, 0x00), RGBColor(0xff, 0xcc, 0x99), RGBColor(0xf0, 0x00, 0xf0), \n",
    "            RGBColor(0x00, 0x33, 0xff), RGBColor(0xff, 0xa5, 0x00), RGBColor(0xff, 0x00, 0x00),\n",
    "            RGBColor(0x40, 0xf0, 0x40)]\n",
    "\n",
    "def splitTxt(arrTxt, key):\n",
    "    newArrTxt = []\n",
    "    for sent in arrTxt:\n",
    "        if key in sent:\n",
    "            arrSplit = sent.split(key)\n",
    "\n",
    "#             print(key, arrSplit)\n",
    "            # The case: kidnap maps in [kidnap]ping and [kidnap]pers.\n",
    "            if arrSplit[0] != '' and len(arrSplit) == 2:\n",
    "                if len(arrSplit[1]) == 0:\n",
    "                    newArrTxt.append(sent)\n",
    "                    continue\n",
    "\n",
    "                if isEnglish(arrSplit[0][-1]) or isEnglish(arrSplit[1][0]):\n",
    "                    newArrTxt.append(sent)\n",
    "                    continue\n",
    "            newArrTxt.append(arrSplit[0])\n",
    "            for idx in range(1, len(arrSplit)):\n",
    "                if isEnglish(arrSplit[idx][0]):\n",
    "                    newArrTxt.append(key + arrSplit[idx])\n",
    "                    continue\n",
    "                newArrTxt.append(key)\n",
    "                newArrTxt.append(arrSplit[idx])\n",
    "        else:\n",
    "            newArrTxt.append(sent)\n",
    "    return newArrTxt\n",
    "    \n",
    "def highlight_run(plainTxt, wordList, outputFile):\n",
    "    arrPara = []\n",
    "#     print(plainTxt)\n",
    "    for paraTxt in plainTxt:\n",
    "        arrTxt = [paraTxt]\n",
    "        for hword in wordList:\n",
    "            arrTxt = splitTxt(arrTxt, hword)\n",
    "#             print(arrTxt)\n",
    "        arrPara.append(arrTxt)\n",
    "        \n",
    "#     print(arrPara)\n",
    "    document = Document()\n",
    "    \n",
    "    tableItems = {}\n",
    "    highlightCnt = 1\n",
    "    for para in arrPara:\n",
    "        docPara = document.add_paragraph(\"\")\n",
    "\n",
    "        for partTxt in para:\n",
    "            if partTxt in wordList:\n",
    "                # get the level of the word\n",
    "                lowerWord = partTxt.lower()\n",
    "                worddic = lemmatizer(lowerWord, dicDf)\n",
    "                level = worddic['level']\n",
    "                if worddic['lemma'] not in tableItems:\n",
    "                    tableItems[worddic['lemma']] = level\n",
    "                    \n",
    "                highlightCnt += 1\n",
    "#                 print(partTxt, level)\n",
    "    \n",
    "#                 docPara.add_run(partTxt).font.highlight_color = colorMap[level]\n",
    "                run = docPara.add_run(partTxt)\n",
    "                run.font.color.rgb = colorMap[level]\n",
    "                if level > 4:\n",
    "                    run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "            \n",
    "            else:\n",
    "                docPara.add_run(partTxt)\n",
    "\n",
    "    # add table ------------------\n",
    "    table = document.add_table(1, 4)\n",
    "\n",
    "    # populate header row --------\n",
    "    heading_cells = table.rows[0].cells\n",
    "    heading_cells[0].text = ''\n",
    "    heading_cells[1].text = 'vocab'\n",
    "    heading_cells[2].text = 'level'\n",
    "    heading_cells[3].text = 'Chinese'\n",
    "\n",
    "    # sort table items\n",
    "    sortedItems = sorted(tableItems.items(), key=lambda kv: kv[1], reverse=True)\n",
    "#     print(sortedItems)\n",
    "    \n",
    "    # add a data row for each item\n",
    "    # ('accord', {'idx': 1, 'level': 6})\n",
    "    idx = 1\n",
    "    for vocab, level in sortedItems:\n",
    "#         print(vocab)\n",
    "        cells = table.add_row().cells\n",
    "        cells[0].text = str(idx)\n",
    "\n",
    "        run1 = cells[1].paragraphs[0].add_run(vocab)\n",
    "        run1.font.color.rgb = colorMap[level]\n",
    "        if level > 4:\n",
    "            run1.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "            \n",
    "        cells[2].paragraphs[0].add_run(str(level)).font.color.rgb = colorMap[level]\n",
    "        cells[3].text = ''\n",
    "    \n",
    "        idx += 1\n",
    "    table.style = 'Table Grid'\n",
    "    \n",
    "    print(highlightCnt)\n",
    "    document.save(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDir(readDir, writeDir):\n",
    "    for root, dirs, files in os.walk(readDir):\n",
    "        for file in sorted(files):\n",
    "            # '.filename.docx': temporary file in Mac\n",
    "            # '~': The file could be temporary opened.\n",
    "            if '.docx' in file and file[0] != '.' and file[0] != '~':\n",
    "                # create new folder if not exist\n",
    "                createDir = '{0}/{1}'.format(writeDir, root[root.rfind('/')+1:])\n",
    "                if not os.path.exists(createDir):\n",
    "                    os.makedirs(createDir)\n",
    "\n",
    "                readFile = os.path.join(root, file)\n",
    "                print(readFile)\n",
    "                writeFile = os.path.join(createDir, file)\n",
    "#                 print(writeFile)\n",
    "\n",
    "                # #VINCENT#, 20190917, To highlight the high level words\n",
    "                wordCnt, plainTxt, wordList = readTxt(readFile)\n",
    "                processedList = processDoc(wordList)\n",
    "                highlight_run(plainTxt, processedList, writeFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/backup_128G/z_repository/Yang_data/學測指考英文/test/aaa/A-099-Q16to20-M清.docx\n",
      "88%  includes un-alpha characters.\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "parseDir('/Volumes/backup_128G/z_repository/Yang_data/學測指考英文/test', \n",
    "         '/Volumes/backup_128G/z_repository/Yang_data/學測指考英文/testOutput')\n",
    "\n",
    "# parseDir(from_dataPath, to_dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
