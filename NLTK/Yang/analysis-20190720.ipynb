{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = '/Volumes/backup_128G/z_repository/Yang_data/20190720'\n",
    "dataPath = 'data'\n",
    "\n",
    "experimentPrePath = '{0}/{1}/實驗組校對y0719/實驗組_前測(experiental-pretest)'.format(basePath, dataPath)\n",
    "experimentPostPath = '{0}/{1}/實驗組校對y0719/實驗組_後測(experiental-posttest)'.format(basePath, dataPath)\n",
    "comparisonPrePath = '{0}/{1}/對照組校對y0719new/對照組_前測(control-pretest)'.format(basePath, dataPath)\n",
    "comparisonPostPath = '{0}/{1}/對照組校對y0719new/對照組_後測(control-posttest)'.format(basePath, dataPath)\n",
    "\n",
    "file_dic = 'permantData/7000Words_20190512_v2.xlsx'\n",
    "to_exppre_doc = 'output/experiment_pre_20190720_v2.xlsx'\n",
    "to_exppost_doc = 'output/experiment_post_20190720_v2.xlsx'\n",
    "to_compre_doc = 'output/comparison_pre_20190720_v2.xlsx'\n",
    "to_compost_doc = 'output/comparison_post_20190720_v2.xlsx'\n",
    "\n",
    "to_level_doc = 'output/level_20190720_v2.xlsx'\n",
    "\n",
    "write_exppre_doc = '{0}/{1}'.format(basePath, to_exppre_doc)\n",
    "write_exppost_doc = '{0}/{1}'.format(basePath, to_exppost_doc)\n",
    "write_compre_doc = '{0}/{1}'.format(basePath, to_compre_doc)\n",
    "write_compost_doc = '{0}/{1}'.format(basePath, to_compost_doc)\n",
    "\n",
    "write_level_doc = '{0}/{1}'.format(basePath, to_level_doc)\n",
    "read_dic = '{0}/{1}'.format(basePath, file_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6813"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicDf = pd.DataFrame()\n",
    "\n",
    "with pd.ExcelFile(read_dic) as reader:\n",
    "    # read sheet by sheet\n",
    "    for sheet in reader.sheet_names:\n",
    "#         print(sheet)\n",
    "        sheetDf = pd.read_excel(reader, sheet, header=None)\n",
    "        sheetDf = sheetDf.fillna(0)\n",
    "\n",
    "        dicDf = dicDf.append(sheetDf, ignore_index=True)\n",
    "\n",
    "# change to lowercase\n",
    "dicDf[0] = dicDf[0].str.lower()\n",
    "len(dicDf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>art.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an</td>\n",
       "      <td>art.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>v.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>v.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbreviation</td>\n",
       "      <td>n.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1  2\n",
       "0             a  art.  1\n",
       "1            an  art.  1\n",
       "2       abandon    v.  4\n",
       "3    abbreviate    v.  6\n",
       "4  abbreviation    n.  6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將英文字 lemmatize（詞形還原）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞性還原 \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wtlem = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def findWord(word, dicDataF):\n",
    "    found = dicDataF.loc[dicDataF[0] == word]\n",
    "    if found.empty == False:\n",
    "        return found.iloc[0][2]\n",
    "    return None\n",
    "    \n",
    "def returnWord(word, level):\n",
    "    return {'lemma':word, 'level':level}\n",
    "\n",
    "def lemmatizer(word, dicDataF):\n",
    "    lowerWord = word.lower()\n",
    "    \n",
    "    found = findWord(lowerWord, dicDataF)\n",
    "    if found != None:\n",
    "        return returnWord(word, found)\n",
    "    \n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.ADJ)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.VERB)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "        \n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.NOUN)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    lemmaWord = wtlem.lemmatize(lowerWord, wordnet.ADV)\n",
    "    if lemmaWord != lowerWord:\n",
    "        found = findWord(lemmaWord, dicDataF)\n",
    "        if found != None:\n",
    "            return returnWord(lemmaWord, found)\n",
    "\n",
    "    return returnWord(word, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀檔；去掉非英文字，回傳 word 字串，包含引用次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-docx \n",
    "import docx\n",
    "\n",
    "def isAlpha(word):\n",
    "    try:\n",
    "        return word.encode('ascii').isalpha()\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    \n",
    "def wordCount(wordList):\n",
    "    wCount = {}\n",
    "    for word in wordList:\n",
    "        if word not in wCount:\n",
    "            wCount[word] = 1\n",
    "        else:\n",
    "            wCount[word] += 1\n",
    "    return wCount\n",
    "    \n",
    "def split2Words(txt):\n",
    "    wordList = []\n",
    "    cleanTxt = txt.replace(',', '').replace('.', '')\n",
    "    splitWords = cleanTxt.split(' ')\n",
    "    for word in splitWords:\n",
    "        if '’' in word or '-' in word:\n",
    "            wordList.append(word)\n",
    "            continue\n",
    "\n",
    "        if isAlpha(word) == True:\n",
    "            wordList.append(word)\n",
    "            \n",
    "    return wordCount(wordList)\n",
    "    \n",
    "def replaceMultiple(mainString, toBeReplaces, newString):\n",
    "#     outString = copy.copy(mainString)\n",
    "    # Iterate over the strings to be replaced\n",
    "    for elem in toBeReplaces :\n",
    "        # Check if string is in the main string\n",
    "        if elem in mainString :\n",
    "            # Replace the string\n",
    "            mainString = mainString.replace(elem, newString)\n",
    "    \n",
    "    return mainString\n",
    "\n",
    "def readtxt(filename):\n",
    "    fullText = []\n",
    "    doc = docx.Document(filename)\n",
    "    tables = doc.tables\n",
    "    for table in tables:\n",
    "        for row in table.rows:\n",
    "            for cell in row.cells:\n",
    "                for paragraph in cell.paragraphs:\n",
    "                    processedPara = replaceMultiple(paragraph.text, ['…', '’', '‘', '-'] , ' ')\n",
    "#                     print(processedPara)\n",
    "                    fullText.append(processedPara)\n",
    "#                     print(paragraph.text)\n",
    "\n",
    "    fullText = ' '.join(fullText)\n",
    "    return split2Words(fullText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀檔列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDoc(wordList):\n",
    "    resWordList = {}\n",
    "    for word in wordList:\n",
    "        lowerWord = word.lower()\n",
    "        worddic = lemmatizer(lowerWord, dicDf)\n",
    "        if worddic['lemma'] not in resWordList:\n",
    "            resWordList[worddic['lemma']] = {'words':word, 'count':wordList[word], \n",
    "                                             'level':worddic['level'], \n",
    "                                             'index':'{0}({1})'.format(worddic['lemma'], wordList[word])}\n",
    "        else:\n",
    "            resWordList[worddic['lemma']]['words'] += ';{0}'.format(word)\n",
    "            resWordList[worddic['lemma']]['count'] += wordList[word]\n",
    "            resWordList[worddic['lemma']]['index'] = '{0}({1})'.format(worddic['lemma'], resWordList[worddic['lemma']]['count'])\n",
    "            if resWordList[worddic['lemma']]['level'] != worddic['level']:\n",
    "                print(\"ERROR!!\", word, worddic)\n",
    "\n",
    "    #     print(word, lemmatizer(word, dicDf))\n",
    "    return resWordList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDir(filedir, outputFile):\n",
    "    levelColumns = []\n",
    "    levelNums = []\n",
    "    levelWords = []\n",
    "    with pd.ExcelWriter(outputFile) as writer:\n",
    "        for root, dirs, files in os.walk(filedir):\n",
    "            for file in sorted(files):\n",
    "                if '.docx' in file and file[0] != '.':\n",
    "                    # The file could be temporary opened.\n",
    "                    if file[0] == '~':\n",
    "                        continue\n",
    "\n",
    "                    sheetName = file.replace('.docx', '')\n",
    "\n",
    "                    filename = os.path.join(root, file)\n",
    "                    print(filename)\n",
    "                    wordList = readtxt(filename)\n",
    "                    processedList = processDoc(wordList)\n",
    "\n",
    "                    # output\n",
    "                    outDf = pd.DataFrame(processedList).T\n",
    "                    outDf.sort_values(by=['level', 'words'], inplace=True)\n",
    "#                     print(outDf.head())\n",
    "\n",
    "                    # list level words\n",
    "                    levelColumns.append(sheetName)\n",
    "\n",
    "                    numWords = {}\n",
    "                    wordWords = {}\n",
    "#                     for level in range(1, 7):\n",
    "                    for level in [0, 1, 2, 3, 4, 5, 6, 9]:\n",
    "                        words = outDf.loc[outDf['level'] == level].index\n",
    "                        count = outDf.loc[outDf['level'] == level]['count'].sum()\n",
    "#                         print(level, count)\n",
    "#                         numWords[level] = len(words)\n",
    "                        numWords[level] = count\n",
    "                        wordWords[level] = ';'.join(words)\n",
    "\n",
    "                    levelNums.append(numWords)\n",
    "                    levelWords.append(wordWords)\n",
    "                    \n",
    "                    # reindex\n",
    "                    outDf.set_index('index', inplace=True)\n",
    "                    \n",
    "                    # write file\n",
    "                    outDf.to_excel(writer, sheetName)\n",
    "                    writer.save()\n",
    "\n",
    "    levelNumDf = pd.DataFrame(levelNums, index=levelColumns)\n",
    "    levelWordsDf = pd.DataFrame(levelWords, index=levelColumns)\n",
    "    return levelNumDf, levelWordsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT01-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT02-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT03-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT04-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT05-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT06-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT07-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT09-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT11-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT12-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT13-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT14-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT15-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT16-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT17-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_前測(control-pretest)/CONT20-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT01-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT02-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT03-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT04-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT05-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT06-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT07-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT09-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT11-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT12-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT13-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT14-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT15-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT16-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT17-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/對照組校對y0719new/對照組_後測(control-posttest)/CONT20-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP01-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP02-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP03-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP04-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP05-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP06-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP07-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP08-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP09-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP10-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP11-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP12-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP13-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP14-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP15-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_前測(experiental-pretest)/EXP16-pre.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/EXP01-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/EXP02-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/EXP03-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/EXP04-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/EXP05-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/EXP06-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP07-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP08-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP09-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP10-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP11-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP12-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP13-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP14-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP15-post.docx\n",
      "/Volumes/backup_128G/z_repository/Yang_data/20190720/data/實驗組校對y0719/實驗組_後測(experiental-posttest)/to be cleaned/EXP16-post.docx\n"
     ]
    }
   ],
   "source": [
    "resMap = []\n",
    "\n",
    "# Comparison Pretest\n",
    "numDf, wordsDf = parseDir(comparisonPrePath, write_compre_doc)\n",
    "resObj = {'name':'COMP-Pre Num', 'data':numDf}\n",
    "resMap.append(resObj)\n",
    "\n",
    "resObj = {'name':'COMP-Pre Words', 'data':wordsDf}\n",
    "resMap.append(resObj)\n",
    "\n",
    "# Comparison Posttest\n",
    "numDf, wordsDf = parseDir(comparisonPostPath, write_compost_doc)\n",
    "resObj = {'name':'COMP-Post Num', 'data':numDf}\n",
    "resMap.append(resObj)\n",
    "\n",
    "resObj = {'name':'COMP-Post Words', 'data':wordsDf}\n",
    "resMap.append(resObj)\n",
    "\n",
    "# Experimental Pretest\n",
    "numDf, wordsDf = parseDir(experimentPrePath, write_exppre_doc)\n",
    "resObj = {'name':'EXP-Pre Num', 'data':numDf}\n",
    "resMap.append(resObj)\n",
    "\n",
    "resObj = {'name':'EXP-Pre Words', 'data':wordsDf}\n",
    "resMap.append(resObj)\n",
    "\n",
    "# Experimental Posttest\n",
    "numDf, wordsDf = parseDir(experimentPostPath, write_exppost_doc)\n",
    "resObj = {'name':'EXP-Post Num', 'data':numDf}\n",
    "resMap.append(resObj)\n",
    "\n",
    "resObj = {'name':'EXP-Post Words', 'data':wordsDf}\n",
    "resMap.append(resObj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write level words/nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(write_level_doc) as writer:\n",
    "    # write file\n",
    "    for row in resMap:\n",
    "        name = row['name']\n",
    "        data = row['data']\n",
    "        \n",
    "        data.to_excel(writer, name)\n",
    "    \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
